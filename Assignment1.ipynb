{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73cbeff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Welcome to the I2IT-NLP Page. \n",
      " Good Morning \t\n",
      "\n",
      "Splitting using whitespece into separate tokens:\n",
      "['Welcome', 'to', 'the', 'I2IT-NLP', 'Page.', 'Good', 'Morning']\n"
     ]
    }
   ],
   "source": [
    "# import WhitespaceTokenizer() method from nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "     \n",
    "# Create a reference variable for Class WhitespaceTokenizer\n",
    "wt = WhitespaceTokenizer()\n",
    "\n",
    "# Create a string input\n",
    "text = \"Welcome to the I2IT-NLP Page. \\n Good Morning \\t\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)     \n",
    "# Use tokenize method\n",
    "tokenized_text = wt.tokenize(text)\n",
    "print(\"\\nSplitting using whitespece into separate tokens:\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78bc88f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Welcome to the I2IT-NLP Page. \n",
      " Good Morning \t\n",
      "\n",
      "Split all punctuation into separate tokens:\n",
      "['Welcome', 'to', 'the', 'I2IT', '-', 'NLP', 'Page', '.', 'Good', 'Morning']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text = \"Welcome to the I2IT-NLP Page. \\n Good Morning \\t\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "result = WordPunctTokenizer().tokenize(text)\n",
    "print(\"\\nSplit all punctuation into separate tokens:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32d1799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome', 'to', 'the', 'I2IT-NLP', 'Page.', 'Good', 'Morning']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    " \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a3d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'is', 'your', 'favourite', 'cryptocurrency', 'influencer', '?', 'ðŸ—£', 'ðŸ†', 'Tag', 'them', 'below', '!', 'ðŸ‘‡']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "sample_tweet = \"Who is your favourite cryptocurrency influencer? ðŸ—£ðŸ† Tag them below! ðŸ‘‡\"\n",
    "print(tweet_tokenize.tokenize(sample_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c6e914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import MWETokenizer() method from nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "#from nltk.tokenize import  word_tokenize\n",
    "\n",
    "# Create a reference variable for Class MWETokenizer\n",
    "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff1b00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Steven_Spielberg', 'is', 'an', 'American', 'writer', 'producer', 'director']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('Steven', 'Spielberg'))\n",
    "tokenizer.tokenize('Steven Spielberg is an American writer producer director '.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31407703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connector\n",
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words =[\"connector\",\"connection\",\"connects\",\"connecting\",\"connected\"]\n",
    "         \n",
    "#Next, we can easily stem by doing something like:\n",
    "for w in example_words:\n",
    "  print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2daed15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ---> generous\n",
      "generate ---> generat\n",
      "generously ---> generous\n",
      "generation ---> generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "words = ['generous','generate','generously','generation']\n",
    "for word in words:\n",
    "    print(word,\"--->\",snowball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26676412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26362b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
